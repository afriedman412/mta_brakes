{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MTA Discovers Twitter\n",
    "In early 2018, the MTA made waves by (finally) investing in social media. The agency made it known they had hired a whole new team with the expressed purpose of better customer service in the digital era. So what would that change look like on Twitter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as pld\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to download every tweet from 2017 to the present which contained \"nyctsubway\" (the official Twitter handle for the subways) using [Get Old Tweets](https://github.com/Jefferson-Henrique/GetOldTweets-python), a straightforward Python wrapper for the Twitter API. It was saved as \"more_tweets_plus.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('more_tweets_plus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this basic function to remove @ mentions, URL's and MTA employee signatures (which take the form \"^XX\") from tweets, as well as converting tweets to all lower-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleany(tweet):\n",
    "    tweet = re.sub('@\\S+','', tweet)\n",
    "    tweet = re.sub('\\^\\S+','', tweet)\n",
    "    tweet = re.sub('https:\\S+','', tweet)\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tweet_clean'] = df['text'].map(cleany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with bad or null tweets, then making 'day' feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df[df.tweet_clean.notnull()]\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "df2['day'] = df2.date.dt.to_period('D').map(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stop words from NLTK and adding overly prominent words discovered during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = nltk.corpus.stopwords.words('english')\n",
    "train_words = ['train', 'trains', 'st', 'av', 'http', 'custhelp']\n",
    "stops = stops+train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to efficiently convert tweets into daily word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pulls 'bag of words' data from one day of tweets\n",
    "def span_vec_day(day, data=df2, n_grams=(1,1), pro='stem', norm=False, binary=False):\n",
    "    \n",
    "    # pulls tweets from 'data' on 'day'\n",
    "    tweets = data.set_index('day').loc[day]['tweet_clean']\n",
    "    \n",
    "    # option to process words by stemming or lemmatizing\n",
    "    if pro == 'stem':\n",
    "        t_pro = tweets.map(stem)\n",
    "    elif pro == 'lem':\n",
    "        t_pro = tweets.map(lemma)\n",
    "    else:\n",
    "        t_pro = tweets\n",
    "        \n",
    "    # vectorize using params set in function call\n",
    "    cv = CountVectorizer(ngram_range=n_grams, stop_words=stops, binary=binary)\n",
    "    v_tweets = cv.fit_transform(t_pro)\n",
    "    \n",
    "    # reconstruct dataframe of word counts\n",
    "    gram = pd.DataFrame(\n",
    "        pd.DataFrame(v_tweets.todense(),\n",
    "                     columns=cv.get_feature_names()).sum().sort_values(ascending=False)\n",
    "        ).rename(columns={0:'count'})\n",
    "    \n",
    "    # option to normalize word counts to length of tweet\n",
    "    if norm:\n",
    "        gram['norm'] = gram['count'].map(lambda x: x/len(tweets))\n",
    "        \n",
    "    return gram\n",
    "\n",
    "# from 'start' to 'end returns top 'n' most common words for each day\n",
    "def vec_day_top_n(n=25, start=False, end=False, data=df2, n_grams=(1,1), pro='stem', norm=False, binary=False):\n",
    "    \n",
    "    # indexes 'data' on day column\n",
    "    d_i = data.set_index('day')\n",
    "    \n",
    "    # uses earliest date and latest date if no 'min' or 'max' specified\n",
    "    if not start:\n",
    "        start = d_i.index.min()\n",
    "    if not end:\n",
    "        end = d_i.index.max()\n",
    "    \n",
    "    # initiate collector dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # iterate through dates in d_i\n",
    "    for d in d_i.index.unique():\n",
    "        \n",
    "        # clear tracked output every ten days\n",
    "        if d[-1] == '1':\n",
    "            clear_output()\n",
    "        \n",
    "        # run 'span_vec_day' on 'd' with specified params\n",
    "        gram = span_vec_day(day=d, data=data, n_grams=n_grams, norm=norm, pro=pro, binary=binary)\n",
    "        \n",
    "        # return dataframe of top 'n' most common words for each day\n",
    "        df_gram = pd.DataFrame(gram).reset_index().rename(columns={'index':'word',0:'count'}).iloc[0:n,:]\n",
    "        \n",
    "        # create 'date' column\n",
    "        df_gram['date'] = d\n",
    "        \n",
    "        # add to collector df\n",
    "        df = pd.concat([df, df_gram])\n",
    "        \n",
    "        # print date for tracking\n",
    "        print(d) \n",
    "    return df.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-11\n",
      "2017-01-10\n",
      "2017-01-09\n",
      "2017-01-08\n",
      "2017-01-07\n",
      "2017-01-06\n",
      "2017-01-05\n"
     ]
    }
   ],
   "source": [
    "# top 25 1-, 2-, and 3-grams for all dates\n",
    "top25_daily_ng1to3 = vec_day_top_n(n=25, n_grams=(1,3), pro=None, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-01\n",
      "2018-06-30\n",
      "2018-06-29\n",
      "2018-06-28\n",
      "2018-06-27\n",
      "2018-06-26\n",
      "2018-06-25\n",
      "2018-06-24\n",
      "2018-06-23\n",
      "2018-06-22\n"
     ]
    }
   ],
   "source": [
    "# top 100 1-grams for all dates\n",
    "top100_daily = vec_day_top_n(n=100, pro=None, norm=True, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to plot seaborn regplots (scatter plot + regression line) of word counts by day. 'regplot_words' contains parameters to specify data, specific words or a word range, normalized counts or raw counts, and to add a vertical line to the plot.\n",
    "\n",
    "'reg9' runs 'regplot_words' for the top `i` to `i+9` words in the provided `data` ie it does a 3x3 grid of regplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regplot of words from vectorized tweets\n",
    "def regplot_words(i=0, j=3, words=None, data=top100_daily, fig=True,\n",
    "              lim=None, line=False, fit=True, norm=False, alpha=0.5):\n",
    "    \n",
    "    # if 'norm' use normalized word counts, otherwise use raw counts\n",
    "    if norm:\n",
    "        y = 'norm'\n",
    "    else:\n",
    "        y = 'count' \n",
    "    \n",
    "    # use word(s) if provided (and convert single word to list) ...\n",
    "    if words is not None:\n",
    "        if type(words) == str:\n",
    "            word_list = [words]\n",
    "        else:\n",
    "            word_list = words\n",
    "    \n",
    "    # ... otherwise create word list from i'th to j'th most common words in data'\n",
    "    else:\n",
    "        word_list = []\n",
    "        for w in data.groupby('word').sum().sort_values(y, ascending=False).iloc[i:j].reset_index()['word']:\n",
    "            word_list.append(w)\n",
    "            \n",
    "    # pull list of unique dates\n",
    "    dates = data.loc[:,'date'].unique()\n",
    "    \n",
    "    # option to make one big plot instead of multiple, evenly-sized plots\n",
    "    if fig:\n",
    "        fig = plt.figure(figsize=(15,7))\n",
    "        \n",
    "    # iterate through 'word_list' ...\n",
    "    for w in word_list:\n",
    "        \n",
    "        # create dataframe of word counts by day, filling in missing dates with 0\n",
    "        d = data[data['word'] == w].sort_values('date')\n",
    "        counts = {'date':[], y:[]}\n",
    "        for d1 in dates:\n",
    "            counts['date'].append(d1)\n",
    "            if d.loc[:,'date'].str.contains(d1).sum() > 0:\n",
    "                counts[y].append(d[d['date'] == d1][y].values[0])\n",
    "            else:\n",
    "                counts[y].append(0)\n",
    "                \n",
    "        # plot word counts by date\n",
    "        # fit regression line if 'fit_reg'\n",
    "        plot_df = pd.DataFrame(counts).reset_index()\n",
    "        sns.regplot(\n",
    "            data=plot_df, x='index', y=y, fit_reg=fit, scatter_kws=({'alpha':alpha, 'color':'orange'}))\n",
    "        \n",
    "        # for scaling x-axis\n",
    "        l = plot_df['index'].max()\n",
    "        \n",
    "        # option to plot a vertical line\n",
    "        if line:\n",
    "            plt.axvline(x=line, linestyle=':')\n",
    "            \n",
    "#         # add legend\n",
    "#         plt.legend(labels=word_list)\n",
    "\n",
    "        # add title\n",
    "        plt.title(s=w)\n",
    "        \n",
    "        # set y-axis for normalized or raw counts\n",
    "        if norm:\n",
    "            plt.ylim(-0.1, 1)\n",
    "        elif lim:\n",
    "            plt.ylim(-10,lim)\n",
    "        plt.xlim(-l*.05,l+(l*0.05))\n",
    "        plt.xlabel('');\n",
    "    \n",
    "# a 3x3 subplot of sns_words for i=i*9, j=(i*9)+9\n",
    "# 'line' 'lim' and 'fit' are are passed to regplot_words\n",
    "\n",
    "def reg9(i, data=top100_daily, lim=None, line=None, fit=True, norm=True, alpha=0.5, title=''):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.suptitle(t=title, size=17, y=0.93)\n",
    "    j = i*9\n",
    "    for n in range(9):\n",
    "        plt.subplot(3,3,n+1)\n",
    "        regplot_words(j+n,j+n+1, data=data, fig=False, lim=lim, line=line, fit=fit, norm=norm, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setting `fit` to 361 because that's 1/1/2018 -- more or less when we expect to see changes in tweet syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg9(0, data=top100_daily, norm=True, alpha=0.2, line=361, fit=True, title='Top 9 1-grams from Top 100 Daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the research\n",
    "This cursory analysis reveals some evident trends in MTA twitter. Use of the term \"due\" drops off almost entirely after 2018 starts, while \"good\" and \"hi\" are on the upswing. More importantly, this is proof of concept. It's anecdotal evidence that we can use this method to look for changes in how the MTA tweets.\n",
    "\n",
    "Transit journalist Aaron W. Gordon noted that subway alerts about emergency break activations seemed to have increased in the last few months. On June 28th, [the MTA noted](https://twitter.com/NYCTSubway/status/1012343209295536128) that they had a new policy to explicity mention when a train's emergency breaks were set off, as opposed to calling such incidents \"mechanical problems\". Gordon asked me to see if I could visualize the change in syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing tweet search for 'nyctsubway' and 'brakes' from 2017 and 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('brakes.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day'] = df.date.dt.to_period('D').map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "df['auth'] = df['permalink'].map(lambda x: re.search(r'(?<=com\\/)\\w.+(?=\\/status\\/)',x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing tweet search for 'nyctsubway' and 'mechanical' from 2017 and 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mech = pd.read_csv('mechanical.csv').dropna()\n",
    "mech['date'] = pd.to_datetime(mech['date'])\n",
    "mech['day'] = mech.date.dt.to_period('D').map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "mech['auth'] = mech['permalink'].map(lambda x: re.search(r'(?<=com\\/)\\w.+(?=\\/status\\/)',x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marking tweets that contain \"breaks were automatically activated\" and which contain the stem \"activ\". Less automated tweets means we have to account for \"activate, activated, activation\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brakes_2017_2018 = df[(df.date.dt.year == 2018) | (df.date.dt.year == 2017)]\n",
    "brakes_2017_2018['bwaa'] = brakes_2017_2018['text'].str.contains('brakes were automatically activated')\n",
    "brakes_2017_2018['act'] = brakes_2017_2018['text'].str.contains('activ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marking tweets that contain \"because of a train with mechanical problems\" and \"due to a train with mechanical problems\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mech_2017_2018 = mech[(mech.date.dt.year == 2018) | (mech.date.dt.year == 2017)]\n",
    "mech_2017_2018['mp'] = mech_2017_2018['text'].str.contains('because of a train with mechanical problems')\n",
    "mp = mech_2017_2018[mech_2017_2018['auth'] == 'NYCTSubway'][['day','mp']].groupby('day').sum().reset_index()\n",
    "mp['day'] = pd.to_datetime(mp['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "due_str = 'due to a train with mechanical problems'\n",
    "mech_2017_2018['due'] = mech_2017_2018['text'].str.contains(due_str)\n",
    "due = mech_2017_2018[mech_2017_2018['auth'] == 'NYCTSubway'][['day','due']].groupby('day').sum().reset_index()\n",
    "due.day = pd.to_datetime(due.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling for viz purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bwaa = brakes_2017_2018[brakes_2017_2018['auth'] == 'NYCTSubway'][['day','bwaa']].groupby('day').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act = brakes_2017_2018[brakes_2017_2018['auth'] == 'NYCTSubway'][['day','act']].groupby('day').sum().reset_index()\n",
    "act.day = pd.to_datetime(act.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bwaa.day = pd.to_datetime(bwaa.day)\n",
    "for d in [bwaa, due, act]:\n",
    "    mp = pd.merge(mp, d, how='outer', on='day').fillna(0)\n",
    "idx = pd.date_range('2017-01-01', '2018-08-05')\n",
    "\n",
    "df_imputed = mp.set_index('day').reindex(idx, fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data to show clear trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_imputed.set_index('index').resample('W').sum().reset_index()\n",
    "\n",
    "kwargs = {'linestyle':':', 'marker':'.', 'mew':'4', 'alpha':0.7}\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "plt.title(s='Euphemisms for emergency brakes used on MTA official Twitter per week', size=16)\n",
    "plt.plot_date(data=d, x='index', y ='due', xdate=True, **kwargs)\n",
    "plt.plot_date(data=d, x='index', y ='mp', xdate=True, **kwargs)\n",
    "plt.plot_date(data=d, x='index', y ='act', xdate=True, **kwargs)\n",
    "plt.yticks(size=14)\n",
    "plt.legend(['...due to a train with mechanical problems','...because of a train with mechanical problems', 'brakes + activate'], fontsize=12)\n",
    "plt.xlabel('Week', size=12)\n",
    "plt.ylabel('Total Mentions', size=12);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
